\begin{appendix}

\chapter{Code}
\section{Feature Selection}
\scriptsize{
\begin{verbatim}
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.ensemble import ExtraTreesClassifier
    from data_preprocessing_unsw import import_and_clean

    ds = import_and_clean("UNSW-NB15_3.csv")

    model = ExtraTreesClassifier()

    model.fit(ds.iloc[:, :-2], ds.iloc[:, -1])

    plt.bar(np.arange(47), model.feature_importances_)
    plt.xlabel("Features")
    plt.ylabel("Importance")
    plt.show()
\end{verbatim}


\section{Data Preprocessing}
\begin{verbatim}
    import pandas as pd
    import numpy as np

    features = pd.read_csv("../dataset/UNSW-NB15_features.csv")

    def import_and_clean(dataset):
        print("importing", dataset, "...")
        data = pd.read_csv("../dataset/" + dataset, low_memory=False)
        data.columns = features.iloc[:, 1]
        print("filling blank entries...")
        data = data.fillna(0)

        print("converting nominal data to numeric...")
        cols = data.select_dtypes('object').columns
        data[cols] = data[cols].apply(lambda x: x.astype('category').cat.codes)

        return data
\end{verbatim}

\section{Evaluation Script}
\begin{verbatim}
    def evaluate(classification_result, labels):
        comparison = list(zip(classification_result, labels))

        result = {
            (0, 0): 0,
            (0, 1): 0,
            (1, 0): 0,
            (1, 1): 0
        }

        for res in comparison:
            result[res] = result[res] + 1

        print("in", classification_result.shape[0], "records, the model predicts")
        print("True Negative:\t", result[(0, 0)])
        print("False Negative:\t", result[(0, 1)])
        print("False Positive:\t", result[(1, 0)])
        print("True Positive:\t", result[(1, 1)])
        accuracy = (result[(1, 1)] + result[(0, 0)]) / (result[(0, 0)] + result[(0, 1)] + result[(1, 0)] + result[(1, 1)])
        print("accuracy = \t", accuracy)
\end{verbatim}

\section{General script format for testing with all features}
\begin{verbatim}
    import time
    import sys
    import graphviz
    from data_preprocessing_unsw import import_and_clean
    from evaluation import evaluate
    from sklearn import tree

    train = import_and_clean("UNSW-NB15_1.csv")
    test = import_and_clean("UNSW-NB15_2.csv")

    dtc = tree.DecisionTreeClassifier(max_depth = int(sys.argv[1]))

    print("training...")
    start = time.time()
    dtc.fit(train.iloc[:, :-2], train.iloc[:, -1])
    end = time.time()

    ttf = (end - start)

    print("testing...")
    start = time.time()
    y_dtc = dtc.predict(test.iloc[:, :-2])
    end = time.time()

    ttp = (end - start)

    print("evaluating...")
    evaluate(y_dtc, test.iloc[:, -1])

    print("ttf = \t\t", ttf)
    print("ttp = \t\t", ttp)

    dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=train.columns[:-2], class_names=['normal', 'attack'], filled=True, special_characters=True)
    graph = graphviz.Source(dot_data)
    graph.render("./results/dtc_clf_md" + str(sys.argv[1]) + "_all_features")
\end{verbatim}

\section{General script format for testing with the two selected features}
\begin{verbatim}
    import time
    import sys
    import graphviz
    from data_preprocessing_unsw import import_and_clean
    from evaluation import evaluate
    from sklearn import tree

    train = import_and_clean("UNSW-NB15_1.csv")
    test = import_and_clean("UNSW-NB15_2.csv")

    dtc = tree.DecisionTreeClassifier(max_depth = int(sys.argv[1]))

    print("training...")
    start = time.time()
    dtc.fit(train.iloc[:, [9, 36]], train.iloc[:, -1])
    end = time.time()

    ttf = (end - start)

    print("testing...")
    start = time.time()
    y_dtc = dtc.predict(test.iloc[:, [9, 36]])
    end = time.time()

    ttp = (end - start)

    print("evaluating...")
    evaluate(y_dtc, test.iloc[:, -1])

    print("ttf = \t\t", ttf)
    print("ttp = \t\t", ttp)

    dot_data = tree.export_graphviz(dtc, out_file=None, feature_names=['sttl', 'ct_state_ttl'], class_names=['normal', 'attack'], filled=True, special_characters=True)
    graph = graphviz.Source(dot_data)
    graph.render("./results/dtc_clf_md" + str(sys.argv[1]) + "_2_features")
\end{verbatim}

\chapter{Trace Files}

\section{General Trace Format}
\begin{verbatim}
$ python decision_tree_all_features.py 2
    importing UNSW-NB15_1.csv ...
    filling blank entries...
    converting nominal data to numeric...
    importing UNSW-NB15_2.csv ...
    filling blank entries...
    converting nominal data to numeric...
    training...
    testing...
    evaluating...
    in 700000 records, the model predicts
    True Negative:	 638630
    False Negative:	 248
    False Positive:	 8621
    True Positive:	 52501
    accuracy = 	 0.98733
    ttf = 		 3.5196142196655273
    ttp = 		 0.29450440406799316
\end{verbatim}

}
\end{appendix}
